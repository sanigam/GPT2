{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://eightportions.com/datasets/Recipes/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'transformers' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/huggingface/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /home/jupyter/GPT2/transformers\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers==4.16.0.dev0) (6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from transformers==4.16.0.dev0) (0.4.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers==4.16.0.dev0) (2.26.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers==4.16.0.dev0) (2021.11.10)\n",
      "Requirement already satisfied: tokenizers>=0.10.1 in /opt/conda/lib/python3.7/site-packages (from transformers==4.16.0.dev0) (0.11.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers==4.16.0.dev0) (1.19.5)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers==4.16.0.dev0) (4.9.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers==4.16.0.dev0) (21.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers==4.16.0.dev0) (3.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers==4.16.0.dev0) (4.62.3)\n",
      "Requirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers==4.16.0.dev0) (0.0.47)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.16.0.dev0) (4.0.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->transformers==4.16.0.dev0) (3.0.6)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers==4.16.0.dev0) (3.6.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.16.0.dev0) (2.0.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.16.0.dev0) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.16.0.dev0) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.16.0.dev0) (3.1)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==4.16.0.dev0) (1.0.1)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==4.16.0.dev0) (8.0.3)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==4.16.0.dev0) (1.16.0)\n",
      "Building wheels for collected packages: transformers\n",
      "  Building wheel for transformers (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for transformers: filename=transformers-4.16.0.dev0-py3-none-any.whl size=3414773 sha256=9b5aab8454b20eaa534673f1517636e609e08ce54660ddf6f3a0e4f197c21235\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-nvmf96mp/wheels/39/86/5c/283acb0825b1597200d7c6e54ed3dd13d4c33e476ae8d572e5\n",
      "Successfully built transformers\n",
      "Installing collected packages: transformers\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.16.0.dev0\n",
      "    Uninstalling transformers-4.16.0.dev0:\n",
      "      Successfully uninstalled transformers-4.16.0.dev0\n",
      "Successfully installed transformers-4.16.0.dev0\n"
     ]
    }
   ],
   "source": [
    "!cd transformers; pip3 install ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  model_demo import demo\n",
    "#demo ('Ingredients: 1 cup flour \\n 1 cup sugar \\n cocoa \\n beer \\n Instructions:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "%matplotlib inline\n",
    "matplotlib.rcParams['figure.dpi'] = 200\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/GPT2\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = json.load(open('./recipes_raw/recipes_raw_nosource_ar.json', 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('rmK12Uau.ntP510KeImX506H6Mr6jTu',\n",
       " {'title': 'Slow Cooker Chicken and Dumplings',\n",
       "  'ingredients': ['4 skinless, boneless chicken breast halves ADVERTISEMENT',\n",
       "   '2 tablespoons butter ADVERTISEMENT',\n",
       "   '2 (10.75 ounce) cans condensed cream of chicken soup ADVERTISEMENT',\n",
       "   '1 onion, finely diced ADVERTISEMENT',\n",
       "   '2 (10 ounce) packages refrigerated biscuit dough, torn into pieces ADVERTISEMENT',\n",
       "   'ADVERTISEMENT'],\n",
       "  'instructions': 'Place the chicken, butter, soup, and onion in a slow cooker, and fill with enough water to cover.\\nCover, and cook for 5 to 6 hours on High. About 30 minutes before serving, place the torn biscuit dough in the slow cooker. Cook until the dough is no longer raw in the center.\\n',\n",
       "  'picture_link': '55lznCYBbs2mT8BTx6BTkLhynGHzM.S'})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(data_dict.items())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "\n",
    "sources , titles, ingredients, instructions = [], [], [], []\n",
    "for source in ['ar', 'epi', 'fn']:\n",
    "    data_dict = json.load(open(f'./recipes_raw/recipes_raw_nosource_{source}.json', 'r'))\n",
    "    for _, recipe in data_dict.items() :\n",
    "        #print (recipe['title'] )\n",
    "        if ('title' in recipe) and ('ingredients' in recipe) and ('instructions' in recipe):\n",
    "            sources.append(source)\n",
    "            titles.append (recipe['title'])\n",
    "            ingredients.append([ ing.replace('ADVERTISEMENT', '') for ing in recipe['ingredients']])\n",
    "            instructions.append ( str(recipe['instructions']).replace('ADVERTISEMENT', '').replace('\\n', ' ') )\n",
    "       \n",
    "df['source'] =  sources \n",
    "df['title'] = titles\n",
    "df['ingredients'] =  ingredients\n",
    "df['instructions'] =  instructions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>title</th>\n",
       "      <th>ingredients</th>\n",
       "      <th>instructions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>55893</th>\n",
       "      <td>epi</td>\n",
       "      <td>Evening in Kingston</td>\n",
       "      <td>[1 1/2 cups amber rum (such as Appleton Estate...</td>\n",
       "      <td>Combine 1 1/2 cups amber rum (such as Appleton...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30022</th>\n",
       "      <td>ar</td>\n",
       "      <td>Easy Cheese Dip</td>\n",
       "      <td>[1/2 cup cream cheese , 1/4 cup sour cream , 1...</td>\n",
       "      <td>Mix cream cheese and sour cream together in a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37042</th>\n",
       "      <td>ar</td>\n",
       "      <td>Maple Apple Bourbon Glaze</td>\n",
       "      <td>[2/3 cup apple juice , 1/3 cup grade B maple s...</td>\n",
       "      <td>Combine apple juice, maple syrup, bourbon, bro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52711</th>\n",
       "      <td>epi</td>\n",
       "      <td>Dark Chocolate Ganache</td>\n",
       "      <td>[2 3/4 cups mini semisweet chocolate chips, 2 ...</td>\n",
       "      <td>1 Put the chocolate chips in a large heatproof...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102870</th>\n",
       "      <td>fn</td>\n",
       "      <td>Buttermilk Southwestern Grilled Chicken</td>\n",
       "      <td>[2 cups buttermilk, 1 lime, zested and juiced,...</td>\n",
       "      <td>Whisk together the buttermilk, lime zest and j...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       source                                    title  \\\n",
       "55893     epi                     Evening in Kingston    \n",
       "30022      ar                          Easy Cheese Dip   \n",
       "37042      ar                Maple Apple Bourbon Glaze   \n",
       "52711     epi                  Dark Chocolate Ganache    \n",
       "102870     fn  Buttermilk Southwestern Grilled Chicken   \n",
       "\n",
       "                                              ingredients  \\\n",
       "55893   [1 1/2 cups amber rum (such as Appleton Estate...   \n",
       "30022   [1/2 cup cream cheese , 1/4 cup sour cream , 1...   \n",
       "37042   [2/3 cup apple juice , 1/3 cup grade B maple s...   \n",
       "52711   [2 3/4 cups mini semisweet chocolate chips, 2 ...   \n",
       "102870  [2 cups buttermilk, 1 lime, zested and juiced,...   \n",
       "\n",
       "                                             instructions  \n",
       "55893   Combine 1 1/2 cups amber rum (such as Appleton...  \n",
       "30022   Mix cream cheese and sour cream together in a ...  \n",
       "37042   Combine apple juice, maple syrup, bourbon, bro...  \n",
       "52711   1 Put the chocolate chips in a large heatproof...  \n",
       "102870  Whisk together the buttermilk, lime zest and j...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(124647, 4)\n"
     ]
    }
   ],
   "source": [
    "display(df.sample(5))\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ingredient_count'] = df['ingredients'].str.len().fillna(0).astype(int)\n",
    "df['instruction_length'] = df['instructions'].str.split().str.len().fillna(0).astype(int)\n",
    "# sns.boxplot(data=df, x='source', y='ingredient_count')\n",
    "#sns.boxplot(data=df, x='source', y='instruction_length', showfliers=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(124647, 6)\n",
      "(121639, 6)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)\n",
    "df = df[(df.ingredient_count >= 3) & (df.instruction_length >= 12) ]\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_token = ' <|endoftext|> '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df ['combined'] = ' \\n Ingredients: \\n ' +  df['ingredients'].str.join(' \\n ') + ' \\n Instructions: \\n ' + df['instructions'] + special_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " Ingredients: \n",
      " 4 skinless, boneless chicken breast halves  \n",
      " 2 tablespoons butter  \n",
      " 2 (10.75 ounce) cans condensed cream of chicken soup  \n",
      " 1 onion, finely diced  \n",
      " 2 (10 ounce) packages refrigerated biscuit dough, torn into pieces  \n",
      "  \n",
      " Instructions: \n",
      " Place the chicken, butter, soup, and onion in a slow cooker, and fill with enough water to cover. Cover, and cook for 5 to 6 hours on High. About 30 minutes before serving, place the torn biscuit dough in the slow cooker. Cook until the dough is no longer raw in the center.  <|endoftext|> \n"
     ]
    }
   ],
   "source": [
    "print(df ['combined'].iloc[0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = df[:120000].combined.values\n",
    "dataset_val = df[120000:].combined.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/dataset_train.txt', 'w') as f :\n",
    "    f.write('\\n'.join(dataset_train))\n",
    "with open('data/dataset_val.txt', 'w') as f :\n",
    "    f.write('\\n'.join(dataset_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('distilgpt2')\n",
    "model = AutoModelForCausalLM.from_pretrained('distilgpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[8888,  318, 3217,   11,  314,  716, 1016,  284,  220]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_text = \"Today is Friday, I am going to \"\n",
    "encoded_prompt = tokenizer.encode(prompt_text, add_special_tokens=False, return_tensors='pt')\n",
    "encoded_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "out_sequences = \\\n",
    "model.generate (input_ids = encoded_prompt, max_length = 200, temperature = .9, top_k=20, top_p=.9, \n",
    "                repetition_penalty= 1, do_sample=True, num_return_sequences=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today is Friday, I am going to   open a new window to all the new   people   on   Facebook,   Twitter,   Tumblr.\n",
      "\n",
      "I hope I can get into a few more   things.\n",
      "-  My wife and I have a   car   on my   car     home in Florida  \n",
      "We are in the middle of the winter when I have a   car         home in Florida  \n",
      "It is a very important time to   open   a new window to all the   people   on   Facebook,   Tumblr, and    Tumblr. I would like to share with you the new   people   on   Facebook,   Tumblr, and    Tumblr.\n",
      "I am  planning to open a  new  window to all the   people   on   Facebook\n",
      "Today is Friday, I am going to                        \n",
      "The most important thing to note, is that when you take a trip to a city where you will find many people that are very similar to you and are quite different than you.\n",
      "In other words, you will find people that are very similar to you and are quite different from you and are quite different from you and are quite different from you and are quite different from you and are quite different from you and are quite different from you and are quite different from you and are quite different from you and are quite different from you and are quite different from you and are quite different from you and are quite different from you and are quite different from you and are quite different from you and are quite different from you and are quite different from you and are quite different from you and are quite different from you and are quite different from you and are\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(out_sequences [0]))\n",
    "print(tokenizer.decode(out_sequences [1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/jupyter/GPT2\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "01/16/2022 22:17:53 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 4, distributed training: False, 16-bits training: False\n",
      "01/16/2022 22:17:53 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=4,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=False,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=experiments/epochs_10/runs/Jan16_22-17-50_gpt2-cuda,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=10.0,\n",
      "optim=OptimizerNames.ADAMW_HF,\n",
      "output_dir=experiments/epochs_10,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=4,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=experiments/epochs_10,\n",
      "save_on_each_node=False,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.STEPS,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/auto/modeling_auto.py:795: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  FutureWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/data/datasets/language_modeling.py:58: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/pytorch/language-modeling/run_mlm.py\n",
      "  FutureWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/trainer.py:1102: FutureWarning: `model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` instead.\n",
      "  FutureWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 40809\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 25510\n",
      "  0%|                                                 | 0/25510 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "{'loss': 2.0919, 'learning_rate': 4.901999215993728e-05, 'epoch': 0.2}          \n",
      "  2%|▋                                    | 500/25510 [07:45<6:20:54,  1.09it/s]Saving model checkpoint to experiments/epochs_10/checkpoint-500\n",
      "Configuration saved in experiments/epochs_10/checkpoint-500/config.json\n",
      "Model weights saved in experiments/epochs_10/checkpoint-500/pytorch_model.bin\n",
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "{'loss': 1.8543, 'learning_rate': 4.8039984319874556e-05, 'epoch': 0.39}        \n",
      "  4%|█▍                                  | 1000/25510 [15:23<6:12:29,  1.10it/s]Saving model checkpoint to experiments/epochs_10/checkpoint-1000\n",
      "Configuration saved in experiments/epochs_10/checkpoint-1000/config.json\n",
      "Model weights saved in experiments/epochs_10/checkpoint-1000/pytorch_model.bin\n",
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "{'loss': 1.7697, 'learning_rate': 4.705997647981184e-05, 'epoch': 0.59}         \n",
      "  6%|██                                  | 1500/25510 [23:02<6:04:41,  1.10it/s]Saving model checkpoint to experiments/epochs_10/checkpoint-1500\n",
      "Configuration saved in experiments/epochs_10/checkpoint-1500/config.json\n",
      "Model weights saved in experiments/epochs_10/checkpoint-1500/pytorch_model.bin\n",
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "{'loss': 1.7122, 'learning_rate': 4.607996863974912e-05, 'epoch': 0.78}         \n",
      "  8%|██▊                                 | 2000/25510 [30:39<6:02:13,  1.08it/s]Saving model checkpoint to experiments/epochs_10/checkpoint-2000\n",
      "Configuration saved in experiments/epochs_10/checkpoint-2000/config.json\n",
      "Model weights saved in experiments/epochs_10/checkpoint-2000/pytorch_model.bin\n",
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "{'loss': 1.6787, 'learning_rate': 4.5099960799686397e-05, 'epoch': 0.98}        \n",
      " 10%|███▌                                | 2500/25510 [38:17<5:48:48,  1.10it/s]Saving model checkpoint to experiments/epochs_10/checkpoint-2500\n",
      "Configuration saved in experiments/epochs_10/checkpoint-2500/config.json\n",
      "Model weights saved in experiments/epochs_10/checkpoint-2500/pytorch_model.bin\n",
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "{'loss': 1.6436, 'learning_rate': 4.411995295962368e-05, 'epoch': 1.18}         \n",
      " 12%|████▏                               | 3000/25510 [45:54<5:42:20,  1.10it/s]Saving model checkpoint to experiments/epochs_10/checkpoint-3000\n",
      "Configuration saved in experiments/epochs_10/checkpoint-3000/config.json\n",
      "Model weights saved in experiments/epochs_10/checkpoint-3000/pytorch_model.bin\n",
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "{'loss': 1.6174, 'learning_rate': 4.313994511956096e-05, 'epoch': 1.37}         \n",
      " 14%|████▉                               | 3500/25510 [53:31<5:34:07,  1.10it/s]Saving model checkpoint to experiments/epochs_10/checkpoint-3500\n",
      "Configuration saved in experiments/epochs_10/checkpoint-3500/config.json\n",
      "Model weights saved in experiments/epochs_10/checkpoint-3500/pytorch_model.bin\n",
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "{'loss': 1.5913, 'learning_rate': 4.2159937279498244e-05, 'epoch': 1.57}        \n",
      " 16%|█████▎                            | 4000/25510 [1:01:07<5:24:59,  1.10it/s]Saving model checkpoint to experiments/epochs_10/checkpoint-4000\n",
      "Configuration saved in experiments/epochs_10/checkpoint-4000/config.json\n",
      "Model weights saved in experiments/epochs_10/checkpoint-4000/pytorch_model.bin\n",
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "{'loss': 1.5747, 'learning_rate': 4.117992943943552e-05, 'epoch': 1.76}         \n",
      " 18%|█████▉                            | 4500/25510 [1:08:44<5:18:18,  1.10it/s]Saving model checkpoint to experiments/epochs_10/checkpoint-4500\n",
      "Configuration saved in experiments/epochs_10/checkpoint-4500/config.json\n",
      "Model weights saved in experiments/epochs_10/checkpoint-4500/pytorch_model.bin\n",
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "{'loss': 1.5634, 'learning_rate': 4.01999215993728e-05, 'epoch': 1.96}          \n",
      " 20%|██████▋                           | 5000/25510 [1:16:20<5:10:52,  1.10it/s]Saving model checkpoint to experiments/epochs_10/checkpoint-5000\n",
      "Configuration saved in experiments/epochs_10/checkpoint-5000/config.json\n",
      "Model weights saved in experiments/epochs_10/checkpoint-5000/pytorch_model.bin\n",
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "{'loss': 1.5377, 'learning_rate': 3.921991375931008e-05, 'epoch': 2.16}         \n",
      " 22%|███████▎                          | 5500/25510 [1:23:57<5:03:23,  1.10it/s]Saving model checkpoint to experiments/epochs_10/checkpoint-5500\n",
      "Configuration saved in experiments/epochs_10/checkpoint-5500/config.json\n",
      "Model weights saved in experiments/epochs_10/checkpoint-5500/pytorch_model.bin\n",
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "{'loss': 1.5341, 'learning_rate': 3.823990591924735e-05, 'epoch': 2.35}         \n",
      " 24%|███████▉                          | 6000/25510 [1:31:33<4:55:48,  1.10it/s]Saving model checkpoint to experiments/epochs_10/checkpoint-6000\n",
      "Configuration saved in experiments/epochs_10/checkpoint-6000/config.json\n",
      "Model weights saved in experiments/epochs_10/checkpoint-6000/pytorch_model.bin\n",
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "{'loss': 1.5181, 'learning_rate': 3.725989807918464e-05, 'epoch': 2.55}         \n",
      " 25%|████████▋                         | 6500/25510 [1:39:10<4:47:56,  1.10it/s]Saving model checkpoint to experiments/epochs_10/checkpoint-6500\n",
      "Configuration saved in experiments/epochs_10/checkpoint-6500/config.json\n",
      "Model weights saved in experiments/epochs_10/checkpoint-6500/pytorch_model.bin\n",
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "{'loss': 1.5118, 'learning_rate': 3.627989023912192e-05, 'epoch': 2.74}         \n",
      " 27%|█████████▎                        | 7000/25510 [1:46:47<4:38:59,  1.11it/s]Saving model checkpoint to experiments/epochs_10/checkpoint-7000\n",
      "Configuration saved in experiments/epochs_10/checkpoint-7000/config.json\n",
      "Model weights saved in experiments/epochs_10/checkpoint-7000/pytorch_model.bin\n",
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "{'loss': 1.4072, 'learning_rate': 1.6679733437867505e-05, 'epoch': 6.66}        \n",
      " 67%|█████████████████████▉           | 17000/25510 [4:18:40<2:08:26,  1.10it/s]Saving model checkpoint to experiments/epochs_10/checkpoint-17000\n",
      "Configuration saved in experiments/epochs_10/checkpoint-17000/config.json\n",
      "Model weights saved in experiments/epochs_10/checkpoint-17000/pytorch_model.bin\n",
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "{'loss': 1.4017, 'learning_rate': 1.569972559780478e-05, 'epoch': 6.86}         \n",
      " 69%|██████████████████████▋          | 17500/25510 [4:26:15<2:00:47,  1.11it/s]Saving model checkpoint to experiments/epochs_10/checkpoint-17500\n",
      "Configuration saved in experiments/epochs_10/checkpoint-17500/config.json\n",
      "Model weights saved in experiments/epochs_10/checkpoint-17500/pytorch_model.bin\n",
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "{'loss': 1.3991, 'learning_rate': 1.4719717757742063e-05, 'epoch': 7.06}        \n",
      " 71%|███████████████████████▎         | 18000/25510 [4:33:49<1:53:25,  1.10it/s]Saving model checkpoint to experiments/epochs_10/checkpoint-18000\n",
      "Configuration saved in experiments/epochs_10/checkpoint-18000/config.json\n",
      "Model weights saved in experiments/epochs_10/checkpoint-18000/pytorch_model.bin\n",
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "{'loss': 1.3941, 'learning_rate': 1.3739709917679342e-05, 'epoch': 7.25}        \n",
      " 73%|███████████████████████▉         | 18500/25510 [4:41:24<1:45:36,  1.11it/s]Saving model checkpoint to experiments/epochs_10/checkpoint-18500\n",
      "Configuration saved in experiments/epochs_10/checkpoint-18500/config.json\n",
      "Model weights saved in experiments/epochs_10/checkpoint-18500/pytorch_model.bin\n",
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "{'loss': 1.3926, 'learning_rate': 1.2759702077616622e-05, 'epoch': 7.45}        \n",
      " 74%|████████████████████████▌        | 19000/25510 [4:48:59<1:38:10,  1.11it/s]Saving model checkpoint to experiments/epochs_10/checkpoint-19000\n",
      "Configuration saved in experiments/epochs_10/checkpoint-19000/config.json\n",
      "Model weights saved in experiments/epochs_10/checkpoint-19000/pytorch_model.bin\n",
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "{'loss': 1.3869, 'learning_rate': 1.17796942375539e-05, 'epoch': 7.64}          \n",
      " 76%|█████████████████████████▏       | 19500/25510 [4:56:36<1:31:02,  1.10it/s]Saving model checkpoint to experiments/epochs_10/checkpoint-19500\n",
      "Configuration saved in experiments/epochs_10/checkpoint-19500/config.json\n",
      "Model weights saved in experiments/epochs_10/checkpoint-19500/pytorch_model.bin\n",
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "{'loss': 1.3947, 'learning_rate': 1.079968639749118e-05, 'epoch': 7.84}         \n",
      " 78%|█████████████████████████▊       | 20000/25510 [5:04:11<1:23:18,  1.10it/s]Saving model checkpoint to experiments/epochs_10/checkpoint-20000\n",
      "Configuration saved in experiments/epochs_10/checkpoint-20000/config.json\n",
      "Model weights saved in experiments/epochs_10/checkpoint-20000/pytorch_model.bin\n",
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "{'loss': 1.3942, 'learning_rate': 9.81967855742846e-06, 'epoch': 8.04}          \n",
      " 80%|██████████████████████████▌      | 20500/25510 [5:11:48<1:15:48,  1.10it/s]Saving model checkpoint to experiments/epochs_10/checkpoint-20500\n",
      "Configuration saved in experiments/epochs_10/checkpoint-20500/config.json\n",
      "Model weights saved in experiments/epochs_10/checkpoint-20500/pytorch_model.bin\n",
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "{'loss': 1.3848, 'learning_rate': 8.83967071736574e-06, 'epoch': 8.23}          \n",
      " 82%|███████████████████████████▏     | 21000/25510 [5:19:25<1:08:21,  1.10it/s]Saving model checkpoint to experiments/epochs_10/checkpoint-21000\n",
      "Configuration saved in experiments/epochs_10/checkpoint-21000/config.json\n",
      "Model weights saved in experiments/epochs_10/checkpoint-21000/pytorch_model.bin\n",
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "{'loss': 1.3872, 'learning_rate': 7.85966287730302e-06, 'epoch': 8.43}          \n",
      " 84%|███████████████████████████▊     | 21500/25510 [5:27:02<1:00:38,  1.10it/s]Saving model checkpoint to experiments/epochs_10/checkpoint-21500\n",
      "Configuration saved in experiments/epochs_10/checkpoint-21500/config.json\n",
      "Model weights saved in experiments/epochs_10/checkpoint-21500/pytorch_model.bin\n",
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "{'loss': 1.3791, 'learning_rate': 6.879655037240299e-06, 'epoch': 8.62}         \n",
      " 86%|██████████████████████████████▏    | 22000/25510 [5:34:39<53:19,  1.10it/s]Saving model checkpoint to experiments/epochs_10/checkpoint-22000\n",
      "Configuration saved in experiments/epochs_10/checkpoint-22000/config.json\n",
      "Model weights saved in experiments/epochs_10/checkpoint-22000/pytorch_model.bin\n",
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "{'loss': 1.387, 'learning_rate': 5.899647197177577e-06, 'epoch': 8.82}          \n",
      " 88%|██████████████████████████████▊    | 22500/25510 [5:42:16<45:44,  1.10it/s]Saving model checkpoint to experiments/epochs_10/checkpoint-22500\n",
      "Configuration saved in experiments/epochs_10/checkpoint-22500/config.json\n",
      "Model weights saved in experiments/epochs_10/checkpoint-22500/pytorch_model.bin\n",
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "{'loss': 1.3826, 'learning_rate': 4.9196393571148575e-06, 'epoch': 9.02}        \n",
      " 90%|███████████████████████████████▌   | 23000/25510 [5:49:53<38:06,  1.10it/s]Saving model checkpoint to experiments/epochs_10/checkpoint-23000\n",
      "Configuration saved in experiments/epochs_10/checkpoint-23000/config.json\n",
      "Model weights saved in experiments/epochs_10/checkpoint-23000/pytorch_model.bin\n",
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "{'loss': 1.3827, 'learning_rate': 3.939631517052137e-06, 'epoch': 9.21}         \n",
      " 92%|████████████████████████████████▏  | 23500/25510 [5:57:30<31:48,  1.05it/s]Saving model checkpoint to experiments/epochs_10/checkpoint-23500\n",
      "Configuration saved in experiments/epochs_10/checkpoint-23500/config.json\n",
      "Model weights saved in experiments/epochs_10/checkpoint-23500/pytorch_model.bin\n",
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "{'loss': 1.3783, 'learning_rate': 2.959623676989416e-06, 'epoch': 9.41}         \n",
      " 94%|████████████████████████████████▉  | 24000/25510 [6:05:07<22:56,  1.10it/s]Saving model checkpoint to experiments/epochs_10/checkpoint-24000\n",
      "Configuration saved in experiments/epochs_10/checkpoint-24000/config.json\n",
      "Model weights saved in experiments/epochs_10/checkpoint-24000/pytorch_model.bin\n",
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "{'loss': 1.3765, 'learning_rate': 1.9796158369266955e-06, 'epoch': 9.6}         \n",
      " 96%|█████████████████████████████████▌ | 24500/25510 [6:12:43<15:14,  1.10it/s]Saving model checkpoint to experiments/epochs_10/checkpoint-24500\n",
      "Configuration saved in experiments/epochs_10/checkpoint-24500/config.json\n",
      "Model weights saved in experiments/epochs_10/checkpoint-24500/pytorch_model.bin\n",
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "{'loss': 1.3744, 'learning_rate': 9.99607996863975e-07, 'epoch': 9.8}           \n",
      " 98%|██████████████████████████████████▎| 25000/25510 [6:20:19<07:42,  1.10it/s]Saving model checkpoint to experiments/epochs_10/checkpoint-25000\n",
      "Configuration saved in experiments/epochs_10/checkpoint-25000/config.json\n",
      "Model weights saved in experiments/epochs_10/checkpoint-25000/pytorch_model.bin\n",
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "{'loss': 1.3836, 'learning_rate': 1.960015680125441e-08, 'epoch': 10.0}         \n",
      "100%|██████████████████████████████████▉| 25500/25510 [6:27:54<00:09,  1.11it/s]Saving model checkpoint to experiments/epochs_10/checkpoint-25500\n",
      "Configuration saved in experiments/epochs_10/checkpoint-25500/config.json\n",
      "Model weights saved in experiments/epochs_10/checkpoint-25500/pytorch_model.bin\n",
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "100%|███████████████████████████████████| 25510/25510 [6:28:03<00:00,  1.19it/s]\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 23283.9832, 'train_samples_per_second': 17.527, 'train_steps_per_second': 1.096, 'train_loss': 1.4831824817455594, 'epoch': 10.0}\n",
      "100%|███████████████████████████████████| 25510/25510 [6:28:03<00:00,  1.10it/s]\n",
      "Saving model checkpoint to experiments/epochs_10\n",
      "Configuration saved in experiments/epochs_10/config.json\n",
      "Model weights saved in experiments/epochs_10/pytorch_model.bin\n",
      "Traceback (most recent call last):\n",
      "  File \"run_lm_finetuning.py\", line 296, in <module>\n",
      "    main()\n",
      "  File \"run_lm_finetuning.py\", line 264, in main\n",
      "    if trainer.is_world_master():\n",
      "AttributeError: 'Trainer' object has no attribute 'is_world_master'\n"
     ]
    }
   ],
   "source": [
    "!bash run_experiments.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save('./trained_models/epochs_10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/jupyter/GPT2\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/jupyter/GPT2\n"
     ]
    }
   ],
   "source": [
    "!pwd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained('/home/jupyter/GPT2/experiments/epochs_10/')\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilgpt2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/dataset_val.txt', 'r') as f:\n",
    "    data = f.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_token =\"<|endoftext|>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " Ingredients: \n",
      " 1 1/2 pounds boneless beef sirloin \n",
      " 1 tablespoon soy sauce \n",
      " 2 tablespoons sesame oil \n",
      " 1 tablespoon rice wine or sherry \n",
      " 1 egg white, lightly beaten \n",
      " 1/2 teaspoon salt \n",
      " 2 tablespoons peanut or corn oil \n",
      " 4 dried red chiles, split \n",
      " 1 tablespoon minced garlic \n",
      " 1/2 tablespoon grated ginger \n",
      " 1 teaspoon Szechwan pepper, toasted and crushed \n",
      " 2 scallions, cut in 1/2-inch pieces \n",
      " 1 red bell pepper, cut in pieces \n",
      " 2 tablespoons soy sauce \n",
      " 3 tablespoons rice wine or sherry \n",
      " 2 tablespoons Chinese black vinegar or balsamic \n",
      " 1 teaspoon sugar \n",
      " 1 cup chicken broth \n",
      " 1 tablespoon cornstarch, dissolved in 2 tablespoons water \n",
      " 1/3 cup roasted peanuts \n",
      " Instructions: \n",
      " Trim fat from the steak and cut into 1-inch cubes. Combine the soy sauce, sesame oil, rice wine/sherry, egg white and salt in a glass bowl. Add the beef and stir to coat. Marinate for 1 hour, covered in the refrigerator. Place peanut/corn oil in a wok, swirling to coat the sides, and place over high heat. Add the chilies and cook until they begin to darken. Add the garlic, ginger and Szechwan pepper; continue to cook to infuse the oil. Add the scallions and bell pepper. Remove the steak from the marinade and add it to the wok. Stir-fry the beef for 3 minutes until brown. Blend in soy sauce, rice wine, Chinese vinegar, sugar and chicken broth. Dissolve the cornstarch slurry and add it to the sauce, stirring, to thicken. Sprinkle in the peanuts and stir to coat. Serve over rice. \n"
     ]
    }
   ],
   "source": [
    "data = data.split(special_token)\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " \n",
      " Ingredients: \n",
      " 1 teaspoon honey \n",
      " 1 teaspoon active dry yeast \n",
      " 2 1/4 cups flour, plus more if needed \n",
      " 1 teaspoon kosher salt \n",
      " Extra-virgin olive oil \n",
      " \n",
      " Instructions: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt_text = data[100].split(\"Instructions:\")[0] + '\\n Instructions: \\n'\n",
    "print(prompt_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_prompt = tokenizer.encode(prompt_text, add_special_tokens=False, return_tensors='pt')\n",
    "#encoded_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "out_sequences = \\\n",
    "model.generate (input_ids = encoded_prompt, max_length = 700, temperature = .9, top_k=20, top_p=.9, \n",
    "                repetition_penalty= 1, do_sample=True, num_return_sequences=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  220,   198,   220,  ..., 19537,   220, 50256],\n",
       "        [  220,   198,   220,  ..., 50256, 50256, 50256],\n",
       "        [  220,   198,   220,  ..., 50256, 50256, 50256],\n",
       "        [  220,   198,   220,  ..., 50256, 50256, 50256]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Output 0 :\n",
      " \n",
      " \n",
      " Ingredients: \n",
      " 1 teaspoon honey \n",
      " 1 teaspoon active dry yeast \n",
      " 2 1/4 cups flour, plus more if needed \n",
      " 1 teaspoon kosher salt \n",
      " Extra-virgin olive oil \n",
      " \n",
      " Instructions: \n",
      " Watch how to make this recipe. Combine the honey, yeast, and 1 1/4 cups flour in a small bowl; stir until smooth. Let sit for about 10 minutes to activate the yeast. Let sit until the yeast is dissolved and foamy, about 30 minutes. Heat a large skillet over medium-high heat. Add 1 1/2 cups olive oil; season with salt. Add the yeast mixture and cook, stirring frequently, until the mixture is bubbling and the liquid has been absorbed, about 2 minutes. Remove from the heat; stir in 1 1/2 cups flour, 1/4 teaspoon kosher salt, 1/4 teaspoon pepper, and 1/4 cup olive oil. Let sit until the dough begins to turn light brown, about 10 minutes. Divide the dough into 4 pieces; shape each into a ball. Transfer the dough to a floured work surface; roll each into a ball about 1/8-inch thick. Cut each into 4 pieces; place on a parchment-lined baking sheet. Cover the rolls with plastic wrap and refrigerate for 30 minutes. Preheat the oven to 375 degrees F. Line 2 baking sheets with parchment paper. Place 1 piece of dough on the bottom half of each roll; brush with olive oil and sprinkle with the remaining 1 1/2 cups flour. Roll the dough around the dough to make a 12-inch round and place on the prepared baking sheets. Brush with oil and sprinkle with salt. Bake until golden brown, about 20 minutes. Photograph by Con Poulos \n",
      "Generated Output 1 :\n",
      " \n",
      " \n",
      " Ingredients: \n",
      " 1 teaspoon honey \n",
      " 1 teaspoon active dry yeast \n",
      " 2 1/4 cups flour, plus more if needed \n",
      " 1 teaspoon kosher salt \n",
      " Extra-virgin olive oil \n",
      " \n",
      " Instructions: \n",
      " In a small bowl, mix together the honey, yeast and 3/4 cup water. Let sit for 5 minutes, then stir in the flour and salt. Cover and let rise until doubled in bulk, about 1 hour. Meanwhile, in a small bowl, mix together the remaining 3/4 cup water and olive oil. Let sit until the dough doubles in bulk, about 1 hour. Punch down the dough and divide into 2 pieces. Form each piece into a ball and place on a lightly floured surface. Divide the dough into 4 pieces and shape into loaves, about 3 inches in diameter. Lightly oil the bottoms of the loaves. Place the loaves on a lightly floured surface. Place the loaves on the lightly floured surface. Cover and let rise until doubled in bulk, about 1 hour. Preheat the oven to 375 degrees F. Heat 1 tablespoon olive oil in a large skillet over medium heat. Add the loaves and cook, turning once, until golden brown, about 5 minutes. Transfer to a wire rack to cool completely. \n",
      "Generated Output 2 :\n",
      " \n",
      " \n",
      " Ingredients: \n",
      " 1 teaspoon honey \n",
      " 1 teaspoon active dry yeast \n",
      " 2 1/4 cups flour, plus more if needed \n",
      " 1 teaspoon kosher salt \n",
      " Extra-virgin olive oil \n",
      " \n",
      " Instructions: \n",
      " Watch how to make this recipe. Special equipment: a small bowl, a pastry brush, or a 2-inch round cookie cutter Preheat the oven to 375 degrees F. Whisk together the honey and yeast in a small bowl until smooth. Whisk in the flour and salt. Add 1/2 cup water and stir until smooth. Whisk in the oil until the dough comes together and starts to pull away from the sides of the bowl, about 3 minutes. Divide the dough in half and form each half into a ball. Wrap each ball in plastic wrap and refrigerate for at least 15 minutes. Position a rack in the center of the oven and preheat to 400 degrees F. Roll the dough out on a lightly floured surface into a 12-inch circle. Cut out circles using a 3-inch round cookie cutter. Using a cookie cutter, cut out rounds, then fill each circle with a tablespoon of dough. Bake until golden brown, 10 to 12 minutes. Let cool on a wire rack. \n",
      "Generated Output 3 :\n",
      " \n",
      " \n",
      " Ingredients: \n",
      " 1 teaspoon honey \n",
      " 1 teaspoon active dry yeast \n",
      " 2 1/4 cups flour, plus more if needed \n",
      " 1 teaspoon kosher salt \n",
      " Extra-virgin olive oil \n",
      " \n",
      " Instructions: \n",
      " In a small bowl, dissolve the honey in the yeast and let stand until foamy, about 10 minutes. In a large bowl, whisk together the flour, salt, olive oil and yeast. Add the flour mixture and knead until just combined. Shape the dough into a log, cover and let rise in a warm place until doubled in bulk, about 1 hour. Preheat the oven to 425 degrees F. On a lightly floured surface, roll the dough out to 1/8-inch thickness. Transfer to a lightly floured surface, and cut into desired shapes. Bake until golden brown, 15 to 20 minutes. \n"
     ]
    }
   ],
   "source": [
    "for i, seq in enumerate(out_sequences):\n",
    "    result = tokenizer.decode(seq)\n",
    "    result = result[:result.index(special_token)]\n",
    "    print (f'Generated Output {i} :' )\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|endoftext|>'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "special_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blueberry\n",
      " cheese\n",
      " crackers\n",
      " pasta sauce\n",
      "\n",
      "Generated Output 0 :\n",
      " \n",
      " Ingredients: \n",
      " blueberry\n",
      " cheese\n",
      " crackers\n",
      " pasta sauce\n",
      " \n",
      " Instructions: \n",
      " In a large pot of boiling salted water cook the blueberries in 2 batches until tender, about 8 minutes. Drain and cool completely. In a bowl stir together the blueberries and the cheese until well blended. Chill the mixture, covered, until it is cold. Remove the cheese from the refrigerator 1 hour before serving. \n",
      "Generated Output 1 :\n",
      " \n",
      " Ingredients: \n",
      " blueberry\n",
      " cheese\n",
      " crackers\n",
      " pasta sauce\n",
      " \n",
      " Instructions: \n",
      " In a large bowl, mix together the blueberries, crackers, and cornflake crumbs. Spread the mixture on top of the blueberry mixture, and refrigerate for at least 2 hours or overnight. Preheat oven to 350 degrees F. In a large skillet over medium heat, melt butter. Add the blueberries and saute until slightly browned. Remove from heat, and set aside. Add the cornflake crumb mixture to the skillet, and cook until the cornflake crumb mixture is cooked through. Remove from heat. Mix in the blueberries and salt, and pepper, to taste. Transfer the mixture to a large baking dish. Bake for 45 minutes, until the cheese is melted and bubbly. \n",
      "Generated Output 2 :\n",
      " \n",
      " Ingredients: \n",
      " blueberry\n",
      " cheese\n",
      " crackers\n",
      " pasta sauce\n",
      " \n",
      " Instructions: \n",
      " Preheat the oven to 400 degrees F. In a small bowl, combine blueberries, sugar, butter and cornstarch. Mix well and set aside. In a large skillet, saute the bacon over medium-high heat until it is crispy. Add the onions and saute until the onions are soft. Stir in the blueberries and saute for 1 minute more. Pour in the blue sauce and mix well. Stir in the flour and cook for 1 minute. Pour the sauce over the blueberries and toss well. \n",
      "Generated Output 3 :\n",
      " \n",
      " Ingredients: \n",
      " blueberry\n",
      " cheese\n",
      " crackers\n",
      " pasta sauce\n",
      " \n",
      " Instructions: \n",
      " Preheat the oven to 400 degrees F. Place a pizza stone in the oven and preheat to 450 degrees F. Spread the blueberry mixture on the bottom of the pizza stone. Top with a layer of crackers. Top with another layer of crackers and a layer of crackers. Top with another layer of crackers and another layer of crackers. Top with a layer of crackers and another layer of crackers. Top with another layer of crackers. Top with another layer of crackers and another layer of crackers. Top with another layer of crackers. Top with another layer of crackers. Top with the last layer of crackers and repeat until all of the crackers are used. Remove the crust from the oven and serve. \n"
     ]
    }
   ],
   "source": [
    "ingredients = 'blueberry\\n cheese\\n crackers\\n pasta sauce\\n'\n",
    "prompt_text = ' \\n Ingredients: \\n ' +  ingredients + ' \\n Instructions: '\n",
    "print (ingredients)\n",
    "encoded_prompt = tokenizer.encode(prompt_text, add_special_tokens=False, return_tensors='pt')\n",
    "out_sequences = \\\n",
    "model.generate (input_ids = encoded_prompt, max_length = 700, temperature = .9, top_k=20, top_p=.9, \n",
    "                repetition_penalty= 1, do_sample=True, num_return_sequences=4)\n",
    "for i, seq in enumerate(out_sequences):\n",
    "    result = tokenizer.decode(seq)\n",
    "    result = result[:result.index(special_token)]\n",
    "    print (f'Generated Output {i} :' )\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cu110.m87",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m87"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
